networks:
  backend:
  caddy:
    external: true

secrets:
  postgres_password:
    file: secrets/postgres/postgres_password
  synapse_signing_key:
    file: secrets/synapse/signing.key
  livekit_api_key:
    file: secrets/livekit/livekit_api_key
  livekit_secret_key:
    file: secrets/livekit/livekit_secret_key

services:
  caddy:
    image: lucaslorentz/caddy-docker-proxy:ci-alpine
    restart: always
    ports:
      - 80:80
      - 443:443
      - 8448:8448
    networks:
      caddy:
      backend:
        aliases:
          - $DOMAIN
          - $HOMESERVER_FQDN
          - $ELEMENT_WEB_FQDN
          - $ELEMENT_CALL_FQDN
          - $MAS_FQDN
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - caddy_data:/data
    environment:
      - CADDY_INGRESS_NETWORKS=caddy

  postgres:
    image: postgres:latest
    restart: unless-stopped
    volumes:
      - ./data/postgres:/var/lib/postgresql/data:rw
      - ./scripts/create-multiple-postgresql-databases.sh:/docker-entrypoint-initdb.d/create-multiple-postgresql-databases.sh
    networks:
      - backend
    environment:
      POSTGRES_MULTIPLE_DATABASES: synapse,mas
      POSTGRES_USER: matrix
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      POSTGRES_INITDB_ARGS: --encoding=UTF8 --locale=C
      PGDATA: /var/lib/postgresql/data/data
    secrets:
      - postgres_password
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U matrix" ]
      start_period: "1s"
      interval: "1s"
      timeout: "5s"

  redis:
    image: redis:latest
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli ping | grep PONG" ]
      interval: 1s
      timeout: 3s
      retries: 5
    networks:
      - backend

  synapse:
    image: ghcr.io/element-hq/synapse:latest
    user: $USER_ID:$GROUP_ID
    restart: unless-stopped
    labels:
      caddy_0: ${HOMESERVER_FQDN}:8448
      caddy_0.reverse_proxy: "{{ upstreams 8008 }}"
      caddy_1: ${DOMAIN}
      caddy_1.@well-known: "match_regex ^/.well-known/(matrix|element)"
      caddy_1.handle: "@well-known"
      caddy_1.handle.root: "* /www/.well-known"
      caddy_1.handle.file_server: ""
    volumes:
      - ./data/synapse:/data/:rw
      - ./data/www/:/www/:ro
    networks:
      - backend
      - caddy
    environment:
      SYNAPSE_CONFIG_DIR: /data
      SYNAPSE_CONFIG_PATH: /data/homeserver.yaml
    secrets:
      - synapse_signing_key
    depends_on:
      redis:
        condition: service_started
      postgres:
        condition: service_healthy
      init:
        condition: service_completed_successfully

  mas:
    image: ghcr.io/element-hq/matrix-authentication-service:latest
    restart: unless-stopped
    volumes:
      - ./data/mas:/data:rw
    labels:
      caddy_0: ${DOMAIN}
      caddy_0.reverse_proxy: "/.well-known/openid-configuration {{ upstreams 8080 }}"
      caddy_1: ${MAS_FQDN}
      caddy_1.reverse_proxy: "{{ upstreams 8080 }}"
      caddy_2: ${HOMESERVER_FQDN}:8448
      caddy_2.@mas: "match_regex /_matrix/client/(.*)/(login|logout|refresh)"
      caddy_2.reverse_proxy: "@mas {{ upstreams 8080}}"
    networks:
      - backend
      - caddy
    # FIXME: do we also need to sync the db?
    command: "server --config=/data/config.yaml"
    depends_on:
      postgres:
        condition: service_healthy
      init:
        condition: service_completed_successfully

  element-web:
    image: vectorim/element-web:latest
    restart: unless-stopped
    labels:
      caddy: ${ELEMENT_WEB_FQDN}
      caddy.reverse_proxy: "{{ upstreams 80 }}"
    healthcheck:
      test: [ "CMD-SHELL", "wget -q -O /dev/null http://localhost:80/version || exit 1" ]
      start_period: "5s"
      interval: "15s"
      timeout: "5s"
    networks:
      - backend
      - caddy
    volumes:
      - ./data/element-web/config.json:/app/config.json
    depends_on:
      init:
        condition: service_completed_successfully

  element-call:
    image: ghcr.io/element-hq/element-call:latest-ci
    restart: unless-stopped
    labels:
      caddy: ${ELEMENT_CALL_FQDN}
      caddy.reverse_proxy: "{{ upstreams 8080 }}"
    networks:
      - backend
      - caddy
    volumes:
      - ./data/element-call/config.json:/app/config.json
    depends_on:
      init:
        condition: service_completed_successfully

  livekit:
    image: livekit/livekit-server:latest
    restart: unless-stopped
    labels:
      caddy: ${LIVEKIT_FQDN}
      caddy.reverse_proxy: "{{ upstreams 7880 }}"
      caddy.reverse_proxy.0_header: "Upgrade {header.upgrade}"
      caddy.reverse_proxy.1_header: "Connection \"upgrade\" "
    volumes:
      - ./data/livekit/config.yaml:/etc/livekit.yaml
    command: --config /etc/livekit.yaml --node-ip ${LIVEKIT_NODE_IP}
    ports:
      # - 7880:7880 # HTTP listener
      - 7881:7881 # TCP WebRTC transport, advertised via SDP
      # TODO: expose livekit-turn on TCP & UDP 443 via nginx
      # At least this would allow UDP turn on port 443 for better perf.

      # You can't expose a massive range here as it literally sets up 10,000 userland listeners, which takes forever
      # and will clash with any existing high-numbered ports.
      # So for now, tunnel everything via TCP 7881. FIXME!
      #- 50000-60000:50000-60000/tcp # TCP media
      #- 50000-60000:50000-60000/udp # UDP media
    networks:
      - backend
      - caddy
    depends_on:
      redis:
        condition: service_started

  livekit-jwt:
    build:
      # evil hack to pull in bash so we can run an entrypoint.sh
      # FIXME: it's a bit wasteful; the alternative would be to modify lk-jwt-service to pick up secrets from disk
      # Another alternative would be to factor out secret generation from compose.yml and create an .env up front
      dockerfile_inline: |
        FROM ghcr.io/element-hq/lk-jwt-service:latest-ci AS builder
        FROM alpine:latest
        RUN apk update && apk add bash
        COPY --from=builder /lk-jwt-service /
    restart: unless-stopped
    labels:
      caddy: ${LIVEKIT_JWT_FQDN}
      caddy.reverse_proxy: "{{ upstreams 8080 }}"
    volumes:
      - ./scripts/livekit-jwt-entrypoint.sh:/entrypoint.sh
    entrypoint: /entrypoint.sh
    env_file: .env
    deploy:
      restart_policy:
        condition: on-failure
    networks:
      - backend
      - caddy
    secrets:
      - livekit_api_key
      - livekit_secret_key
    depends_on:
      livekit:
        condition: service_started

  synapse-generic-worker-1:
    image: ghcr.io/element-hq/synapse:latest
    user: $USER_ID:$GROUP_ID
    restart: unless-stopped
    entrypoint: [ "/start.py", "run", "--config-path=/data/homeserver.yaml", "--config-path=/data/workers/synapse-generic-worker-1.yaml" ]
    labels:
      caddy: ${HOMESERVER_FQDN}:8448
      caddy.@sync: "match_regex ^/_matrix/client/(r0|v3)/sync$"
      caddy.@events: "match_regex ^/_matrix/client/(api/v1|r0|v3)/events$"
      caddy.@initial-sync: "match_regex ^/_matrix/client/(api/v1|r0|v3)/initialSync$"
      caddy.@initial-sync-rooms: "match_regex ^/_matrix/client/(api/v1|r0|v3)/rooms/[^/]+/initialSync$"
      caddy.1_reverse_proxy: "@sync {{ upstreams 8081 }}"
      caddy.2_reverse_proxy: "@events {{ upstreams 8081 }}"
      caddy.3_reverse_proxy: "@initial-sync {{ upstreams 8081 }}"
      caddy.4_reverse_proxy: "@initial-sync-rooms {{ upstreams 8081 }}"
    healthcheck:
      test: [ "CMD-SHELL", "curl -fSs http://localhost:8081/health || exit 1" ]
      start_period: "5s"
      interval: "15s"
      timeout: "5s"
    networks:
      - backend
      - caddy
    volumes:
      - ./data/synapse:/data:rw
    environment:
      SYNAPSE_WORKER: synapse.app.generic_worker
    secrets:
      - synapse_signing_key
    depends_on:
      - synapse

  synapse-federation-sender-1:
    image: ghcr.io/element-hq/synapse:latest
    user: $USER_ID:$GROUP_ID
    restart: unless-stopped
    entrypoint: [ "/start.py", "run", "--config-path=/data/homeserver.yaml", "--config-path=/data/workers/synapse-federation-sender-1.yaml" ]
    healthcheck:
      disable: true
    networks:
      - backend
    volumes:
      - ./data/synapse:/data:rw
    environment:
      SYNAPSE_WORKER: synapse.app.federation_sender
    secrets:
      - synapse_signing_key
    depends_on:
      - synapse

volumes:
  caddy_data:
